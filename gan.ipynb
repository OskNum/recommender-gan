{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "681137b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bdc958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(pred, customers, n):\n",
    "    n_cust = customers.shape[0]\n",
    "    cust_sims = np.empty(n_cust)\n",
    "    \n",
    "    for i in range(n_cust):\n",
    "        cosine_sim = dot(pred, customers.values[i])/(norm(pred)*norm(customers.values[i]))\n",
    "        cust_sims[i] = cosine_sim\n",
    "        \n",
    "        \n",
    "    return cust_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de00756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, product_dims, customer_dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        leakyReLU_slope = 0.1\n",
    "        dropout = 0.1\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(product_dims, 64),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "            nn.Linear(64, customer_dims),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, product_vector):\n",
    "        return self.model(product_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0af3b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, customer_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(customer_dims, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, customer_vector):\n",
    "        return self.model(customer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3417dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "transactions = pd.read_csv('data/transactions_1.csv')[['PRODUCT_ID', 'CUSTOMER_ID']]\n",
    "products = pd.read_csv('data/products.csv')\n",
    "customers = pd.read_csv('data/customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83ae697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products data\n",
    "prods_final = pd.concat([\n",
    "    pd.get_dummies(products['CATEGORY'])\n",
    "], axis=1)\n",
    "prods_final['price'] = products['PRICE']/products['PRICE'].max()\n",
    "prods_final['PRODUCT_ID'] = products['ID'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ac08c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_final = pd.concat([\n",
    "    pd.get_dummies(customers['GENDER'])[['Female', 'Male']],\n",
    "    pd.get_dummies(customers['COUNTRY']),\n",
    "    pd.get_dummies(customers['TYPE'])\n",
    "], axis=1)\n",
    "customers_final['age'] = ((datetime.today() - pd.to_datetime(customers['DOB']))/timedelta(days=365)).round()/100\n",
    "customers_final['CUSTOMER_ID'] = customers['ID'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ba7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_final = transactions.merge(customers_final, how='left', on='CUSTOMER_ID') \\\n",
    "    .drop(columns=['CUSTOMER_ID']) \\\n",
    "    .groupby('PRODUCT_ID') \\\n",
    "    .mean() \\\n",
    "    .reset_index(drop=False)\n",
    "\n",
    "merged = prods_final.merge(customers_final, on='PRODUCT_ID')\n",
    "X = merged.iloc[:,0:len(prods_final.columns)].drop(columns='PRODUCT_ID').reset_index(drop=True)\n",
    "y = merged.iloc[:,len(prods_final.columns):len(merged.columns)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b88e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prods_final = transactions[0:100000].merge(prods_final, how='left', on='PRODUCT_ID') \\\n",
    "    .drop(columns=['PRODUCT_ID']) \\\n",
    "    .groupby('CUSTOMER_ID') \\\n",
    "    .mean() \\\n",
    "    .reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ce56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = customers_final.merge(prods_final, on='CUSTOMER_ID')\n",
    "X = merged.iloc[:,0:len(customers_final.columns)].drop(columns='CUSTOMER_ID').reset_index(drop=True)\n",
    "y = merged.iloc[:,len(customers_final.columns):len(merged.columns)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ce8be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nans_mask = X.isna().all(axis=1)\n",
    "X = X[~X_nans_mask]\n",
    "y = y[~X_nans_mask]\n",
    "\n",
    "y_nans_mask = y.isna().all(axis=1)\n",
    "X = X[~y_nans_mask].reset_index(drop=True)\n",
    "y = y[~y_nans_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "# X = transactions.merge(prods_final, how='left', on='PRODUCT_ID').drop(columns=['PRODUCT_ID', 'CUSTOMER_ID'])\n",
    "# y = transactions.merge(customers_final, how='left', on='CUSTOMER_ID').drop(columns=['PRODUCT_ID', 'CUSTOMER_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_nans_mask = X.isna().all(axis=1)\n",
    "X = X[~X_nans_mask]\n",
    "y = y[~X_nans_mask]\n",
    "\n",
    "y_nans_mask = y.isna().all(axis=1)\n",
    "X = X[~y_nans_mask].reset_index(drop=True)\n",
    "y = y[~y_nans_mask].reset_index(drop=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3205b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1\n",
    "batch_size = 64\n",
    "lr = 0.00001\n",
    "num_epochs = 100\n",
    "product_dims = len(X.columns)\n",
    "customer_dims = len(y.columns)\n",
    "disc_input_dims = product_dims + customer_dims\n",
    "#fixed_noise = torch.randn((batch_size, product_dims)).to(device)\n",
    "#fixed_noise = input_data[200:205]\n",
    "step = 0\n",
    "\n",
    "disc = Discriminator(disc_input_dims).to(device)\n",
    "gen = Generator(product_dims, customer_dims).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "train_inds = random.sample(list(X.index.values), int(train_size*len(X.index)))\n",
    "input_data = torch.tensor(X.iloc[train_inds].values.astype(np.float32))\n",
    "true_positive = torch.tensor(y.iloc[train_inds].values.astype(np.float32))\n",
    "labels = torch.ones_like(input_data)\n",
    "train_dataset = TensorDataset(\n",
    "    input_data,\n",
    "    true_positive\n",
    ")\n",
    "loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64727a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] \\ Step [1/418] \\ Loss D: 0.696, Loss G: 0.736\n",
      "tensor([[0.488, 0.499, 0.544, 0.473, 0.493, 0.492, 0.537, 0.520, 0.496, 0.527, 0.470, 0.493, 0.464, 0.505, 0.539, 0.470, 0.524, 0.502, 0.522, 0.486, 0.476],\n",
      "        [0.489, 0.496, 0.538, 0.476, 0.490, 0.493, 0.532, 0.516, 0.498, 0.522, 0.472, 0.493, 0.463, 0.504, 0.537, 0.475, 0.525, 0.492, 0.521, 0.484, 0.480],\n",
      "        [0.490, 0.502, 0.543, 0.473, 0.494, 0.491, 0.535, 0.526, 0.488, 0.524, 0.468, 0.497, 0.467, 0.504, 0.538, 0.473, 0.521, 0.501, 0.519, 0.488, 0.477],\n",
      "        [0.487, 0.499, 0.542, 0.473, 0.497, 0.486, 0.531, 0.522, 0.495, 0.521, 0.473, 0.492, 0.467, 0.502, 0.532, 0.466, 0.524, 0.499, 0.515, 0.477, 0.473],\n",
      "        [0.490, 0.508, 0.545, 0.472, 0.500, 0.491, 0.530, 0.531, 0.491, 0.526, 0.471, 0.485, 0.471, 0.503, 0.537, 0.472, 0.521, 0.505, 0.512, 0.484, 0.470]])\n",
      "Epoch [1/100] \\ Step [1/418] \\ Loss D: 0.683, Loss G: 0.746\n",
      "Epoch [2/100] \\ Step [1/418] \\ Loss D: 0.636, Loss G: 0.820\n",
      "Epoch [3/100] \\ Step [1/418] \\ Loss D: 0.545, Loss G: 0.958\n",
      "Epoch [4/100] \\ Step [1/418] \\ Loss D: 0.464, Loss G: 1.061\n",
      "Epoch [5/100] \\ Step [1/418] \\ Loss D: 0.480, Loss G: 0.969\n",
      "tensor([[0.140, 0.128, 0.366, 0.072, 0.063, 0.199, 0.520, 0.818, 0.107, 0.087, 0.093, 0.175, 0.110, 0.209, 0.125, 0.063, 0.237, 0.132, 0.095, 0.090, 0.438],\n",
      "        [0.115, 0.108, 0.344, 0.050, 0.044, 0.174, 0.551, 0.857, 0.076, 0.062, 0.068, 0.136, 0.081, 0.176, 0.095, 0.040, 0.232, 0.103, 0.066, 0.064, 0.408],\n",
      "        [0.123, 0.119, 0.355, 0.058, 0.046, 0.185, 0.545, 0.843, 0.079, 0.072, 0.069, 0.150, 0.088, 0.172, 0.108, 0.043, 0.229, 0.116, 0.073, 0.070, 0.395],\n",
      "        [0.106, 0.093, 0.343, 0.046, 0.037, 0.168, 0.528, 0.867, 0.072, 0.056, 0.063, 0.129, 0.076, 0.152, 0.096, 0.035, 0.210, 0.097, 0.063, 0.058, 0.422],\n",
      "        [0.104, 0.100, 0.331, 0.045, 0.040, 0.154, 0.558, 0.846, 0.073, 0.058, 0.058, 0.138, 0.077, 0.178, 0.085, 0.037, 0.218, 0.100, 0.058, 0.063, 0.405]])\n",
      "Epoch [6/100] \\ Step [1/418] \\ Loss D: 0.558, Loss G: 0.858\n",
      "Epoch [7/100] \\ Step [1/418] \\ Loss D: 0.633, Loss G: 0.784\n",
      "Epoch [8/100] \\ Step [1/418] \\ Loss D: 0.677, Loss G: 0.733\n",
      "Epoch [9/100] \\ Step [1/418] \\ Loss D: 0.714, Loss G: 0.705\n",
      "Epoch [10/100] \\ Step [1/418] \\ Loss D: 0.734, Loss G: 0.671\n",
      "tensor([[    0.003,     0.002,     0.013,     0.001,     0.000,     0.078,     0.065,     0.149,     0.002,     0.001,     0.001,     0.005,     0.002,     0.005,     0.001,\n",
      "             0.000,     0.012,     0.002,     0.001,     0.001,     0.518],\n",
      "        [    0.002,     0.001,     0.009,     0.000,     0.000,     0.065,     0.056,     0.113,     0.001,     0.001,     0.001,     0.004,     0.002,     0.004,     0.001,\n",
      "             0.000,     0.009,     0.002,     0.001,     0.001,     0.504],\n",
      "        [    0.003,     0.002,     0.012,     0.001,     0.001,     0.074,     0.076,     0.140,     0.002,     0.001,     0.002,     0.005,     0.003,     0.006,     0.002,\n",
      "             0.001,     0.013,     0.003,     0.001,     0.001,     0.503],\n",
      "        [    0.003,     0.001,     0.010,     0.001,     0.000,     0.075,     0.064,     0.139,     0.002,     0.001,     0.001,     0.004,     0.002,     0.004,     0.001,\n",
      "             0.000,     0.011,     0.002,     0.001,     0.001,     0.501],\n",
      "        [    0.002,     0.001,     0.010,     0.001,     0.000,     0.072,     0.068,     0.129,     0.001,     0.001,     0.001,     0.005,     0.002,     0.004,     0.001,\n",
      "             0.000,     0.011,     0.002,     0.001,     0.001,     0.516]])\n",
      "Epoch [11/100] \\ Step [1/418] \\ Loss D: 0.713, Loss G: 0.650\n",
      "Epoch [12/100] \\ Step [1/418] \\ Loss D: 0.691, Loss G: 0.670\n",
      "Epoch [13/100] \\ Step [1/418] \\ Loss D: 0.641, Loss G: 0.706\n",
      "Epoch [14/100] \\ Step [1/418] \\ Loss D: 0.646, Loss G: 0.739\n",
      "Epoch [15/100] \\ Step [1/418] \\ Loss D: 0.636, Loss G: 0.779\n",
      "tensor([[    0.000,     0.000,     0.002,     0.000,     0.000,     0.057,     0.425,     0.159,     0.000,     0.000,     0.000,     0.001,     0.001,     0.001,     0.000,\n",
      "             0.000,     0.003,     0.001,     0.000,     0.000,     0.747],\n",
      "        [    0.002,     0.001,     0.007,     0.000,     0.000,     0.103,     0.458,     0.213,     0.001,     0.001,     0.001,     0.004,     0.002,     0.003,     0.001,\n",
      "             0.000,     0.010,     0.003,     0.001,     0.001,     0.702],\n",
      "        [    0.001,     0.001,     0.005,     0.000,     0.000,     0.089,     0.432,     0.209,     0.001,     0.000,     0.001,     0.002,     0.001,     0.002,     0.001,\n",
      "             0.000,     0.008,     0.002,     0.000,     0.001,     0.708],\n",
      "        [    0.000,     0.000,     0.002,     0.000,     0.000,     0.051,     0.435,     0.150,     0.000,     0.000,     0.000,     0.001,     0.001,     0.001,     0.000,\n",
      "             0.000,     0.003,     0.001,     0.000,     0.000,     0.762],\n",
      "        [    0.001,     0.001,     0.006,     0.000,     0.000,     0.096,     0.389,     0.232,     0.001,     0.000,     0.001,     0.002,     0.002,     0.003,     0.001,\n",
      "             0.000,     0.008,     0.003,     0.001,     0.001,     0.721]])\n",
      "Epoch [16/100] \\ Step [1/418] \\ Loss D: 0.652, Loss G: 0.753\n",
      "Epoch [17/100] \\ Step [1/418] \\ Loss D: 0.645, Loss G: 0.756\n",
      "Epoch [18/100] \\ Step [1/418] \\ Loss D: 0.549, Loss G: 0.883\n",
      "Epoch [19/100] \\ Step [1/418] \\ Loss D: 0.572, Loss G: 0.871\n",
      "Epoch [20/100] \\ Step [1/418] \\ Loss D: 0.639, Loss G: 0.788\n",
      "tensor([[    0.003,     0.001,     0.003,     0.000,     0.000,     0.311,     0.414,     0.225,     0.000,     0.000,     0.001,     0.002,     0.017,     0.002,     0.001,\n",
      "             0.000,     0.350,     0.124,     0.000,     0.001,     0.545],\n",
      "        [    0.004,     0.001,     0.003,     0.000,     0.000,     0.317,     0.425,     0.253,     0.000,     0.000,     0.001,     0.002,     0.017,     0.002,     0.001,\n",
      "             0.000,     0.359,     0.118,     0.000,     0.001,     0.538],\n",
      "        [    0.002,     0.001,     0.002,     0.000,     0.000,     0.285,     0.431,     0.246,     0.000,     0.000,     0.000,     0.001,     0.014,     0.002,     0.000,\n",
      "             0.000,     0.357,     0.103,     0.000,     0.001,     0.548],\n",
      "        [    0.003,     0.001,     0.003,     0.000,     0.000,     0.312,     0.408,     0.249,     0.001,     0.000,     0.001,     0.002,     0.017,     0.002,     0.001,\n",
      "             0.000,     0.346,     0.112,     0.000,     0.001,     0.541],\n",
      "        [    0.004,     0.001,     0.004,     0.000,     0.000,     0.304,     0.438,     0.273,     0.001,     0.000,     0.001,     0.003,     0.020,     0.003,     0.001,\n",
      "             0.000,     0.353,     0.131,     0.001,     0.001,     0.559]])\n",
      "Epoch [21/100] \\ Step [1/418] \\ Loss D: 0.602, Loss G: 0.785\n",
      "Epoch [22/100] \\ Step [1/418] \\ Loss D: 0.570, Loss G: 0.842\n",
      "Epoch [23/100] \\ Step [1/418] \\ Loss D: 0.467, Loss G: 1.003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m opt_disc\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m lossD\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mopt_disc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train Generator min log(1 - D(G(z))) -> max log(D(G(z)))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m output \u001b[38;5;241m=\u001b[39m disc(disc_input_fake)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:132\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:79\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     params_with_grad\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_sparse\u001b[49m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m     grads\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=180, profile='full')\n",
    "\n",
    "n_total_steps = len(loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (input_prod, true_positive) in enumerate(loader):\n",
    "        input_prod = input_prod.to(device)\n",
    "        batch_size = input_prod.shape[0]\n",
    "        \n",
    "        # Train Discriminator: max log(D(real)) + log(1 - D(G(z)))\n",
    "        #noise = torch.randn(batch_size, product_dims)\n",
    "        disc_input_tp = torch.cat((true_positive, input_prod), 1)\n",
    "        disc_tp = disc(disc_input_tp).view(-1)\n",
    "        lossD_tp = criterion(disc_tp, torch.ones_like(disc_tp))\n",
    "        \n",
    "        fake = gen(input_prod)\n",
    "        disc_input_fake = torch.cat((fake, input_prod), 1)\n",
    "        disc_fake = disc(disc_input_fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_tp + lossD_fake)/2\n",
    "        opt_disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # Train Generator min log(1 - D(G(z))) -> max log(D(G(z)))\n",
    "        output = disc(disc_input_fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        if batch_idx == 0:# or ((batch_idx + 1) % 50 == 0):\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] \\ \"\n",
    "                f\"Step [{batch_idx + 1}/{n_total_steps}] \\ \"\n",
    "                f\"Loss D: {lossD:.3f}, Loss G: {lossG:.3f}\"\n",
    "            )\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    random_inds = random.sample(list(range(len(input_data))), 5)\n",
    "                    sample_inputs = input_data[random_inds]\n",
    "                    fake = gen(sample_inputs)\n",
    "                    print(fake[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3efd7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = gen(input_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea19fe4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.13820108e-02, 1.44382866e-05, 9.90198896e-05, ...,\n",
       "        8.21392860e-06, 3.06224247e-05, 7.97897637e-01],\n",
       "       [1.32895112e-02, 2.21742102e-05, 1.37409268e-04, ...,\n",
       "        1.20443901e-05, 4.46451704e-05, 7.92871714e-01],\n",
       "       [1.48815271e-02, 1.72994933e-05, 1.29967608e-04, ...,\n",
       "        1.20420473e-05, 4.87234538e-05, 7.74703920e-01],\n",
       "       ...,\n",
       "       [1.24373361e-02, 2.15632354e-05, 1.40539865e-04, ...,\n",
       "        1.32176674e-05, 4.24373975e-05, 8.02903116e-01],\n",
       "       [1.59358568e-02, 3.84141749e-05, 2.61922221e-04, ...,\n",
       "        2.77221043e-05, 7.82632051e-05, 7.73363829e-01],\n",
       "       [1.04056085e-02, 1.37029920e-05, 1.19411576e-04, ...,\n",
       "        9.56533859e-06, 4.07898842e-05, 8.10565829e-01]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1744c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(preds[0], customers_final.drop(columns='CUSTOMER_ID'), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f9ba4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'customers_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcustomers_final\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUSTOMER_ID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'customers_final' is not defined"
     ]
    }
   ],
   "source": [
    "customers_final.drop(columns='CUSTOMER_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aaf79fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def most_similar(pred, customers):\n",
    "    \n",
    "similarities = cosine_similarity(\n",
    "    preds,\n",
    "    y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "410b0081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Sweden</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>Bargain hunter</th>\n",
       "      <th>Impulse</th>\n",
       "      <th>Lookers</th>\n",
       "      <th>Loyal</th>\n",
       "      <th>Need-based</th>\n",
       "      <th>New</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26699</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26700</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26701</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26702</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26703</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26704 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Female   Male  Sweden     UK     US  Bargain hunter  Impulse  Lookers   \n",
       "0       False   True    True  False  False           False    False    False  \\\n",
       "1       False   True   False  False   True           False    False    False   \n",
       "2        True  False   False   True  False           False    False    False   \n",
       "3        True  False   False  False   True           False    False    False   \n",
       "4       False   True   False  False   True           False    False    False   \n",
       "...       ...    ...     ...    ...    ...             ...      ...      ...   \n",
       "26699    True  False   False   True  False           False    False    False   \n",
       "26700    True  False   False  False   True           False    False    False   \n",
       "26701   False   True    True  False  False           False    False    False   \n",
       "26702    True  False    True  False  False           False    False    False   \n",
       "26703    True  False    True  False  False           False    False    False   \n",
       "\n",
       "       Loyal  Need-based    New   age  \n",
       "0      False        True  False  0.27  \n",
       "1      False        True  False  0.55  \n",
       "2       True       False  False  0.55  \n",
       "3      False        True  False  0.32  \n",
       "4      False       False   True  0.22  \n",
       "...      ...         ...    ...   ...  \n",
       "26699  False        True  False  0.33  \n",
       "26700  False        True  False  0.28  \n",
       "26701  False        True  False  0.58  \n",
       "26702  False       False   True  0.21  \n",
       "26703  False        True  False  0.31  \n",
       "\n",
       "[26704 rows x 12 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400e5412",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msimilarities\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarities' is not defined"
     ]
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07f73830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_weights = {}\n",
    "negative_weights = {}\n",
    "for c in class_names:\n",
    "    positive_weights[c] = train_df.shape[0]/(2*np.count_nonzero(train_df[c]==1))\n",
    "    negative_weights[c] = train_df.shape[0]/(2*np.count_nonzero(train_df[c]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa7dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(products['CATEGORY']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([\n",
    "    pd.get_dummies(products['CATEGORY'])\n",
    "], axis=1)\n",
    "X['price'] = products['PRICE']/products['PRICE'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "073f218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135, 117, 88, 82, 86, 0, 21, 18, 65, 87, 65, 95.7], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive.numpy().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236faed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1588b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
